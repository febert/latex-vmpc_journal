\section{Video Prediction for Control}

\subsection{Video Prediction via Pixel Transformations}
\label{sec:model}
Visual MPC requires a model that can effectively predict the motion of the selected pixels $\pixel_0^{(1)}, \dots, \pixel_0^{(P)}$ up to $T$ steps into the future (except for the classifier-based cost function described in section \ref{subsec:class_cost}).
In this work, we extend the model proposed in \cite{finn_nips}, where this flow prediction capability emerges implicitly, and therefore no external pixel motion supervision is required. Future images are generated by transforming past observations. The model uses stacked convolutional LSTMs that predict a collection of pixel transformations at each time step, with corresponding composition masks. In this model, the previous image $I_t$ is transformed by $N$ separate transformations, and all of the transformed images $\tilde{I}_t^{(i)}$ are composited together according to weights obtained from the predicted masks. Intuitively, each transformation corresponds to the motion of a different object in the scene, and each mask corresponds to the spatial extents of that object. Let us denote the $N$ transformed images as $\tilde{I}_t^{(1)}, ..., \tilde{I}_t^{(N)}$, and the predicted masks as $\mathbf{M}_1, ...\mathbf{M}_N$, where each 1-channel mask is the same size as the image. The next image prediction is then computed by compositing the images together using the masks: $\hat{I}_{t+1} = \sum_{i=1}^N \tilde{I}_t^{(i)} \mathbf{M}_i$. To predict multiple time steps into the future, the model is applied recursively. These transformations can be represented as convolutions, where each pixel in the transformed image is formed by applying a convolution kernel to the previous one. This method can represent a wide range of local transformations. When these convolution kernels are normalized, they can be interpreted as transition probabilities, allowing us to make probabilistic predictions about future locations of individual pixels. To predict the future positions of the designated pixels $d^{(i)}$, the same transformations which are used for the images are applied to $P_{t,d^{(i)}}$ such that $P_{t+1,d^{(i)}} = \frac{1}{P_s}\sum_{i=1}^N \tilde P^{(i)}_{t,d^{(i)}} \mathbf{M}_i $, where $\frac{1}{P_s}$ is a normalizing constant to ensure that $P_{t+1,d^{(i)}}$ adds up to 1 over the spatial dimension of the image. Since this prior predictive model outputs a single image at each time step, it is unable to track pixel positions through occlusions. Therefore, this model is only suitable for planning motions where the user-selected pixels are not occluded during the manipulation, which restricts its use in cluttered environments or with multiple selected pixels. In the next section, we discuss our proposed occlusion-aware model, which lifts this limitation by employing temporal skip connections.

\subsection{Skip Connection Neural Advection Model}
\label{sec:occlusion_model}
To enable effective tracking of objects through occlusions, we propose an extension to the dynamic neural advection (DNA) model~\cite{finn_nips} that incorporates temporal skip connections. This model uses a similar multilayer convolutional LSTM structure; however, the transformations are now conditioned on a context of previous images. In the most generic version, this involves conditioning the transformation at time $t$ on all of the previous images $I_0,...I_{t}$, though in practice we found that a greatly simplified version of this model performed just as well in practice. We will therefore first present the generic model, and then describe the practical simplifications. We refer to this model as the skip connection neural advection model (SNA), since it handles occlusions by copying pixels from prior images in the history such that when a pixel is occluded (e.g., by the robot arm or by another object) it can still reappear later in the sequence. When predicting the next image $\hat{I}_{t+1}$, the generic SNA model transforms each image in the history according to a different transformation and with different masks to produce $\hat{I}_{t+1}$ (see \autoref{fig:general_model} in the appendix):
\begin{equation}
\hat{I}_{t+1} =  \sum_{j=t-T}^{t} \sum_{i=1}^{N} \mathbf{M}_{i,j} \tilde{I}_{j}^{(i)}
\label{eqn:general_model}
\end{equation}
In the case where $t < T$, negative values of $j$ simply reuse the first image in the sequence. This generic formulation can be computationally expensive, since the number of masks and transformations scales with $T \times N$. A more tractable model, which we found works comparably well in practice in our robotic manipulation setting, assumes that occluded objects are typically static throughout the prediction horizon. This assumption allows us to dispense the intermediate transformations and only provide a skip connection from the very first image in the sequence, which is also the only real image, since all of the subsequent images are predicted by the model:
\begin{equation}
\hat{I}_{t+1} =  I_0 \mathbf{M}_{N+1} +  \sum_{i=1}^{N} \tilde{I}_t^{(i)} \mathbf{M}_i
\label{eqn:simplemodel}
\end{equation}
This model only needs to output $N+1$ masks. We observed similar prediction performance when using a transformed initial image $\tilde{I}_0$ in place of $I_0$, and therefore used the simplified model in \autoref{eqn:simplemodel} in all of our experiments. We provide an example of the model recovering from occlusion in \autoref{fig:pix_reappear}. In the figure, the arm moves in front of the designated pixel, marked in blue in \autoref{fig:desig_pix_bluedot}. The graphs in \autoref{fig:pix_reqppear_graph} show the predicted probability of the designated pixel, which is stationary during the entire motion, being at its original position at each step. Precisely when the arm occludes the designated pixel, the pixel's probability of being at this point decreases. This indicates that the model is `unsure' where this pixel is. When the arm unoccludes the designated pixel, it should become visible again, and the probability of the designated pixel being at its original position should go up. In the case of the DNA model and its variants~\cite{finn_nips}, the probability mass does not increase after the object reappears. This is because the DNA model cannot recover information about object that it has `overwritten' during its predictions.
Consequently, for these models, the probability stays low and \emph{moves with the arm}, which causes the model to believe that the occluded pixel also moves with the arm. We identified this as one of the major causes of planning failure when using the prior models. By contrast, in the case of our SNA model, the probability of the correct pixel position increases again rapidly right after the arm unoccludes the object. Furthermore, the probability of the unoccluded object's position becomes increasingly sharp at its original position as the arm moves further away.