\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{tdgammon,atari,e2e,alphago}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual MPC generalizes to a wide range of tasks and objects, allowing flexibility in goal specification and both rigid and deformable objects not seen during training. Each row shows a different example trajectory. From left to right, we show the task definition, the video-predictions for the planned motion, and the actual executions. Here a task is defined by moving an object marked by a designated pixel, shown by a red dot to a goal position shown by a green dot. A goal-image with the desired goal-configuration can be provided both in combination with a designated pixel or instead of it. Best viewed in PDF.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example_traj}{{1}{1}{Visual MPC generalizes to a wide range of tasks and objects, allowing flexibility in goal specification and both rigid and deformable objects not seen during training. Each row shows a different example trajectory. From left to right, we show the task definition, the video-predictions for the planned motion, and the actual executions. Here a task is defined by moving an object marked by a designated pixel, shown by a red dot to a goal position shown by a green dot. A goal-image with the desired goal-configuration can be provided both in combination with a designated pixel or instead of it. Best viewed in PDF.\relax }{figure.caption.1}{}}
\citation{foresight,sna,ebert2018robustness,flo}
\citation{lillicrap2015continuous,sutton1998reinforcement}
\citation{chentanez2005intrinsically,pathak2017curiosity}
\citation{andrychowicz2017hindsight}
\citation{chua2018deep,deisenroth2013survey}
\citation{deisenroth2011pilco}
\citation{nagabandi2017neural}
\citation{watter2015embed}
\citation{alexey,ha2018world,atarioh}
\citation{zhang2018solar}
\citation{agrawal2016learning,nair2017combining,ropes}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Overview of visual MPC. At training time (top) interaction data is collected autonomously and used to train a video-prediction model. At test time (bottom) this model is used for samnpling-based planning. In this work we discuss three different choices for the planning objective.}\relax }}{2}{figure.caption.2}}
\newlabel{fig:overview}{{2}{2}{\small {Overview of visual MPC. At training time (top) interaction data is collected autonomously and used to train a video-prediction model. At test time (bottom) this model is used for samnpling-based planning. In this work we discuss three different choices for the planning objective.}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{sec:rel_work}{{2}{2}{Related Work}{section.2}{}}
\citation{mottaghi2016happens,lerrel,google_handeye,calandra2017feeling,pinto2016curious}
\citation{greg_kahn_uncertainty,crashing}
\citation{atarioh,recurrentsimulators}
\citation{bootsetal,finn_nips,video_pixel_networks}
\citation{beyond_mse,finn_nips,video_pixel_networks}
\citation{zhang2018solar,kurutach2018learning}
\citation{beyond_mse,convlstm,vondrick}
\citation{prednet,dynamic_filter_networks}
\citation{se3}
\citation{video_pixel_networks,scott_reed}
\citation{finn_nips,dynamic_filter_networks}
\citation{finn_nips}
\@writefile{toc}{\contentsline {section}{\numberline {3}Overview}{3}{section.3}}
\newlabel{sec:prelim}{{3}{3}{Overview}{section.3}{}}
\newlabel{sec:vmpc}{{3}{3}{Overview}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Computation graph of the video-prediction model. Time goes from left to right, $a_t$ are the actions, $h_t$ are the hidden states in the recurrent neural network, $\mathaccentV {hat}05E{F}_{t+1 \leftarrow t}$ is a 2D-warping field, $I_t$ are real images, and $\mathaccentV {hat}05E{I}_t$ are predicted images, $\mathcal  {L}$ is a pairwise training-loss.}\relax }}{3}{figure.caption.3}}
\newlabel{fig:prediction_model}{{3}{3}{\small {Computation graph of the video-prediction model. Time goes from left to right, $a_t$ are the actions, $h_t$ are the hidden states in the recurrent neural network, $\hat {F}_{t+1 \leftarrow t}$ is a 2D-warping field, $I_t$ are real images, and $\hat {I}_t$ are predicted images, $\mathcal {L}$ is a pairwise training-loss.}\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Video Prediction for Control}{3}{section.4}}
\newlabel{sec:model}{{4}{3}{Video Prediction for Control}{section.4}{}}
\citation{zhou2016view}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Forward pass through the recurrent SNA model based on \autoref  {eqn:simplemodel}. The red arrow indicates where the image from the first time step $I_0$ is concatenated with the transformed images $\mathaccentV {hat}05E{F}_{t+1 \leftarrow t} \diamond \mathaccentV {hat}05E{I}_t $ multiplying each channel with a separate mask to produce the predicted frame for step $t+1$.}\relax }}{4}{figure.caption.4}}
\newlabel{fig:occlusion_model}{{4}{4}{\small {Forward pass through the recurrent SNA model based on \autoref {eqn:simplemodel}. The red arrow indicates where the image from the first time step $I_0$ is concatenated with the transformed images $\hat {F}_{t+1 \leftarrow t} \diamond \hat {I}_t $ multiplying each channel with a separate mask to produce the predicted frame for step $t+1$.}\relax }{figure.caption.4}{}}
\newlabel{simple_dna}{{2}{4}{Video Prediction for Control}{equation.4.2}{}}
\newlabel{subsec:pixel_trafo}{{4}{4}{Video Prediction for Control}{figure.caption.4}{}}
\newlabel{eqn:prob_forward}{{3}{4}{Video Prediction for Control}{equation.4.3}{}}
\newlabel{eqn:simplemodel}{{5}{4}{Video Prediction for Control}{equation.4.5}{}}
\citation{foresight}
\citation{foresight}
\newlabel{fig:Ng1}{{6a}{5}{Skip connection neural advection (SNA) does not erase or move objects in the background\relax }{figure.caption.6}{}}
\newlabel{sub@fig:Ng1}{{a}{5}{Skip connection neural advection (SNA) does not erase or move objects in the background\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Top rows: Predicted images of arm moving \textit  {in front of} green object with designated pixel (as indicated in \autoref  {fig:desig_pix_bluedot}). Bottom rows: Predicted probability distributions $P_{d}(t)$ of designated pixel obtained by repeatedly applying transformations.\relax }}{5}{figure.caption.6}}
\newlabel{fig:pix_reappear}{{6}{5}{Top rows: Predicted images of arm moving \textit {in front of} green object with designated pixel (as indicated in \autoref {fig:desig_pix_bluedot}). Bottom rows: Predicted probability distributions $P_{d}(t)$ of designated pixel obtained by repeatedly applying transformations.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The blue dot indicates the designated pixel\relax }}{5}{figure.caption.5}}
\newlabel{fig:desig_pix_bluedot}{{5}{5}{The blue dot indicates the designated pixel\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Planning Cost Functions}{5}{section.5}}
\newlabel{sec:cost}{{5}{5}{Planning Cost Functions}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Pixel-Distance based Cost}{5}{subsection.5.1}}
\newlabel{subsec:pixel_dist_cost}{{5.1}{5}{Pixel-Distance based Cost}{subsection.5.1}{}}
\newlabel{eq:cost}{{6}{5}{Pixel-Distance based Cost}{equation.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Registration-Based Cost}{5}{subsection.5.2}}
\newlabel{subsec:reg_cost}{{5.2}{5}{Registration-Based Cost}{subsection.5.2}{}}
\citation{meister2017unflow}
\citation{meister2017unflow}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Closed loop control is achieved by registering the current image $I_t$ globally to the first frame $I_0$ and the goal image $I_g$. In this example registration to $I_0$ succeeds while registration to $I_g$ fails since the object in $I_g$ is too far away.}  \relax }}{6}{figure.caption.7}}
\newlabel{fig:reg_single}{{7}{6}{\small {Closed loop control is achieved by registering the current image $I_t$ globally to the first frame $I_0$ and the goal image $I_g$. In this example registration to $I_0$ succeeds while registration to $I_g$ fails since the object in $I_g$ is too far away.}  \relax }{figure.caption.7}{}}
\newlabel{fig:discrete}{{8b}{6}{\small {Training usage.}\relax }{figure.caption.8}{}}
\newlabel{sub@fig:discrete}{{b}{6}{\small {Training usage.}\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {(a) At test time the registration network registers the current image $I_t$ to the start image $I_0$ (top) and goal image $I_g$ (bottom), inferring the flow-fields $\mathaccentV {hat}05E{F}_{0 \leftarrow t}$ and $\mathaccentV {hat}05E{F}_{g \leftarrow t}$. (b) The registration network is trained by warping images from randomly selected timesteps along a trajectory to each other. }\relax }}{6}{figure.caption.8}}
\newlabel{fig:registration_arch}{{8}{6}{\small {(a) At test time the registration network registers the current image $I_t$ to the start image $I_0$ (top) and goal image $I_g$ (bottom), inferring the flow-fields $\hat {F}_{0 \leftarrow t}$ and $\hat {F}_{g \leftarrow t}$. (b) The registration network is trained by warping images from randomly selected timesteps along a trajectory to each other. }\relax }{figure.caption.8}{}}
\newlabel{eqn:warped_pos}{{9}{6}{Registration-Based Cost}{equation.5.9}{}}
\newlabel{eqn:cost_avg}{{10}{6}{Registration-Based Cost}{equation.5.10}{}}
\citation{maml}
\citation{caml}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Outputs of registration network. The first row shows the timesteps from left to right of a robot picking and moving a red bowl, the second row shows each image warped to the initial image via registration, and the third row shows the same for the goal image. A successful registration in this visualization would result in images that closely resemble the start- or goal image. In the first row, the locations where the designated pixel of the start image $d_0$ and the goal image $d_g$ are found are marked with red and blue crosses, respectively. It can be seen that the registration to the start image (red cross) is failing in the second to last time step, while the registration to the goal image (blue cross) succeeds for all time steps. The numbers in red, in the upper left corners indicate the trade off factors $\lambda $ between the views and are used as weighting factors for the planning cost. (Best viewed in PDF)}\relax }}{7}{figure.caption.9}}
\newlabel{fig:tracking_overtime}{{9}{7}{\small {Outputs of registration network. The first row shows the timesteps from left to right of a robot picking and moving a red bowl, the second row shows each image warped to the initial image via registration, and the third row shows the same for the goal image. A successful registration in this visualization would result in images that closely resemble the start- or goal image. In the first row, the locations where the designated pixel of the start image $d_0$ and the goal image $d_g$ are found are marked with red and blue crosses, respectively. It can be seen that the registration to the start image (red cross) is failing in the second to last time step, while the registration to the goal image (blue cross) succeeds for all time steps. The numbers in red, in the upper left corners indicate the trade off factors $\lambda $ between the views and are used as weighting factors for the planning cost. (Best viewed in PDF)}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Classifier-Based Cost Functions}{7}{subsection.5.3}}
\newlabel{subsec:class_cost}{{5.3}{7}{Classifier-Based Cost Functions}{subsection.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  We propose a framework for quickly specifying visual goals. Our goal classifier is meta-trained with positive and negative examples for diverse tasks (left), which allows it to meta-learn that some factors matter for goals (e.g., relative positions of objects), while some do not (e.g. position of the arm). At meta-test time, this classifier can learn goals for new tasks from a few of examples of success (right - the goal is to place the fork to the right of the plate). The cost can be derived from the learned goal classifier for use with visual MPC.\relax }}{7}{figure.caption.10}}
\newlabel{fig:cls_fig}{{10}{7}{\small We propose a framework for quickly specifying visual goals. Our goal classifier is meta-trained with positive and negative examples for diverse tasks (left), which allows it to meta-learn that some factors matter for goals (e.g., relative positions of objects), while some do not (e.g. position of the arm). At meta-test time, this classifier can learn goals for new tasks from a few of examples of success (right - the goal is to place the fork to the right of the plate). The cost can be derived from the learned goal classifier for use with visual MPC.\relax }{figure.caption.10}{}}
\citation{ADAM}
\citation{grasping_fetal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}When to use which Cost Function?}{8}{subsection.5.4}}
\newlabel{subsec:cost_discuission}{{5.4}{8}{When to use which Cost Function?}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Trajectory Optimizer}{8}{section.6}}
\newlabel{sec:optimizer}{{6}{8}{Trajectory Optimizer}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Trajecotry Optimization in Visual MPC\relax }}{8}{algorithm.1}}
\newlabel{sarsalambdafa}{{1}{8}{Trajecotry Optimization in Visual MPC\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Custom Action Sampling Distributions}{8}{section.7}}
\newlabel{sec:system}{{7}{8}{Custom Action Sampling Distributions}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Robot setup, with 2 standard web-cams arranged at different viewing angles.}\relax }}{8}{figure.caption.11}}
\newlabel{fig:robot_setup}{{11}{8}{\small {Robot setup, with 2 standard web-cams arranged at different viewing angles.}\relax }{figure.caption.11}{}}
\citation{foresight}
\citation{foresight}
\@writefile{toc}{\contentsline {section}{\numberline {8}Multi-View Visual MPC}{9}{section.8}}
\newlabel{sec:multiview}{{8}{9}{Multi-View Visual MPC}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Experimental Evaluation}{9}{section.9}}
\newlabel{sec:experiments}{{9}{9}{Experimental Evaluation}{section.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results for multi-objective pushing on 8 object/goal configurations with 2 seen and 2 novel objects. Values indicate improvement in distance from starting position, higher is better. Units are pixels in the 64x64 images.\relax }}{9}{table.caption.12}}
\newlabel{table:mult_obj}{{1}{9}{Results for multi-objective pushing on 8 object/goal configurations with 2 seen and 2 novel objects. Values indicate improvement in distance from starting position, higher is better. Units are pixels in the 64x64 images.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Comparing Video Prediction Architectures}{9}{subsection.9.1}}
\newlabel{subsec:sna_experiments}{{9.1}{9}{Comparing Video Prediction Architectures}{subsection.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Evaluating Registration-based Cost Functions}{9}{subsection.9.2}}
\newlabel{susbsec:reg_cost_exp}{{9.2}{9}{Evaluating Registration-based Cost Functions}{subsection.9.2}{}}
\citation{babenko2009visual}
\citation{sna}
\citation{babenko2009visual}
\citation{dsae}
\citation{dsae}
\citation{dsae}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  Success rate for long-distance pushing benchmark with 20 different object/goal configurations and short-distance benchmark with 15 object/goal configurations. Success is defined as bringing the object closer than 15 pixels to the goal, which corresponds to around $7.5cm$.\relax }}{10}{table.caption.13}}
\newlabel{table:res_long_short}{{2}{10}{\small Success rate for long-distance pushing benchmark with 20 different object/goal configurations and short-distance benchmark with 15 object/goal configurations. Success is defined as bringing the object closer than 15 pixels to the goal, which corresponds to around $7.5cm$.\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  Object arrangement performance of our goal classifier with distractor objects and with two tasks. The left shows a subset of the 5 positive examples that are provided for inferring the goal classifier(s), while the right shows the robot executing the specified task(s) via visual planning.\relax }}{10}{figure.caption.14}}
\newlabel{fig:cls_results}{{12}{10}{\small Object arrangement performance of our goal classifier with distractor objects and with two tasks. The left shows a subset of the 5 positive examples that are provided for inferring the goal classifier(s), while the right shows the robot executing the specified task(s) via visual planning.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Evaluating Classifier-based Cost Function}{10}{subsection.9.3}}
\newlabel{subsec:eval_classifier}{{9.3}{10}{Evaluating Classifier-based Cost Function}{subsection.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Evaluating Multi-Task Performance}{10}{subsection.9.4}}
\newlabel{subsec:multi_task_bench}{{9.4}{10}{Evaluating Multi-Task Performance}{subsection.9.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  Quantitative performance of visual planning for object rearrangement tasks across different goal specification methods: our meta-learned classifier, DSAE\nobreakspace  {}\cite  {dsae}, and pixel error. Where possible, we include break down the cause of failures into errors caused by inaccurate prediction or planning and those caused by an inaccurate goal classifier.\relax }}{11}{figure.caption.15}}
\newlabel{fig:cls_charts}{{13}{11}{\small Quantitative performance of visual planning for object rearrangement tasks across different goal specification methods: our meta-learned classifier, DSAE~\cite {dsae}, and pixel error. Where possible, we include break down the cause of failures into errors caused by inaccurate prediction or planning and those caused by an inaccurate goal classifier.\relax }{figure.caption.15}{}}
\newlabel{subsec:cloth_folding_data}{{9.4}{11}{Evaluating Multi-Task Performance}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Visual MPC successfully solves a wide variety of tasks including multi-objective tasks, such as placing an object on a plate (row 5 and 6), object positioning with obstacle avoidance (row 7) and folding shorts (row 8). \relax }}{11}{figure.caption.16}}
\newlabel{fig:tile_2}{{14}{11}{Visual MPC successfully solves a wide variety of tasks including multi-objective tasks, such as placing an object on a plate (row 5 and 6), object positioning with obstacle avoidance (row 7) and folding shorts (row 8). \relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results for a combined benchmark of 10 hard object pushing and grasping tasks, along with 6 cloth folding tasks. Values indicate the percentage of trials which ended with the object closer than a threshold distance (measured in pixels) to the designated goal. Higher is better.\relax }}{11}{table.caption.17}}
\newlabel{table:cloth_folding}{{3}{11}{Results for a combined benchmark of 10 hard object pushing and grasping tasks, along with 6 cloth folding tasks. Values indicate the percentage of trials which ended with the object closer than a threshold distance (measured in pixels) to the designated goal. Higher is better.\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion}{11}{section.10}}
\bibstyle{IEEEtran}
\bibdata{mybib}
\bibcite{tdgammon}{1}
\bibcite{atari}{2}
\bibcite{e2e}{3}
\bibcite{alphago}{4}
\bibcite{foresight}{5}
\bibcite{sna}{6}
\bibcite{ebert2018robustness}{7}
\bibcite{flo}{8}
\bibcite{lillicrap2015continuous}{9}
\bibcite{sutton1998reinforcement}{10}
\bibcite{chentanez2005intrinsically}{11}
\bibcite{pathak2017curiosity}{12}
\bibcite{andrychowicz2017hindsight}{13}
\bibcite{chua2018deep}{14}
\bibcite{deisenroth2013survey}{15}
\bibcite{deisenroth2011pilco}{16}
\bibcite{nagabandi2017neural}{17}
\bibcite{watter2015embed}{18}
\bibcite{alexey}{19}
\bibcite{ha2018world}{20}
\bibcite{atarioh}{21}
\bibcite{zhang2018solar}{22}
\bibcite{agrawal2016learning}{23}
\bibcite{nair2017combining}{24}
\bibcite{ropes}{25}
\bibcite{mottaghi2016happens}{26}
\bibcite{lerrel}{27}
\bibcite{google_handeye}{28}
\bibcite{calandra2017feeling}{29}
\bibcite{pinto2016curious}{30}
\bibcite{greg_kahn_uncertainty}{31}
\bibcite{crashing}{32}
\bibcite{recurrentsimulators}{33}
\bibcite{bootsetal}{34}
\bibcite{finn_nips}{35}
\bibcite{video_pixel_networks}{36}
\bibcite{beyond_mse}{37}
\bibcite{kurutach2018learning}{38}
\bibcite{convlstm}{39}
\bibcite{vondrick}{40}
\@writefile{toc}{\contentsline {section}{References}{12}{section*.18}}
\bibcite{prednet}{41}
\bibcite{dynamic_filter_networks}{42}
\bibcite{se3}{43}
\bibcite{scott_reed}{44}
\bibcite{zhou2016view}{45}
\bibcite{meister2017unflow}{46}
\bibcite{maml}{47}
\bibcite{caml}{48}
\bibcite{ADAM}{49}
\bibcite{grasping_fetal}{50}
\bibcite{babenko2009visual}{51}
\bibcite{dsae}{52}
\bibcite{todorov2012mujoco}{53}
\citation{todorov2012mujoco}
\citation{sna}
\@writefile{toc}{\contentsline {section}{Biographies}{13}{IEEEbiography.0}}
\@writefile{toc}{\contentsline {subsection}{Frederik Ebert}{13}{IEEEbiography.1}}
\@writefile{toc}{\contentsline {subsection}{Chelsea Finn}{13}{IEEEbiography.2}}
\@writefile{toc}{\contentsline {subsection}{Annie Xie}{13}{IEEEbiography.3}}
\@writefile{toc}{\contentsline {subsection}{Sudeep Dasari}{13}{IEEEbiography.4}}
\@writefile{toc}{\contentsline {subsection}{Alex Lee}{13}{IEEEbiography.5}}
\@writefile{toc}{\contentsline {subsection}{Sergey Levine}{13}{IEEEbiography.6}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Improvements of the CEM-Optimizer}{13}{section*.19}}
\newlabel{sec:cem_improv}{{A}{13}{\appendixname \nobreakspace \thesectiondis \\* Improvements of the CEM-Optimizer}{section*.19}{}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  B: Experimental Setup}{13}{section*.20}}
\newlabel{sec:experiment_setup}{{B}{13}{\appendixname \nobreakspace \thesectiondis \\* Experimental Setup}{section*.20}{}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  C: Experimental Evaluation}{13}{section*.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Experiments with multiple-objects rearrangement with occlusion}{13}{subsection.Appendix.C.1}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  D: Simulated Experiments}{13}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Applying our method to a pushing task. In the first 3 time instants the object behaves unexpectedly, moving down. The tracking then allows the robot to retry, allowing it to eventually bring the object to the goal.}\relax }}{14}{figure.caption.22}}
\newlabel{fig:push_retry}{{15}{14}{\small {Applying our method to a pushing task. In the first 3 time instants the object behaves unexpectedly, moving down. The tracking then allows the robot to retry, allowing it to eventually bring the object to the goal.}\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \relax \fontsize  {9bp}{10bp}\selectfont  {Retrying behavior of our method combining prehensile and non-prehensile manipulation. In the first 4 time instants shown the robot pushes the object. It then loses the object, and decides to grasp it pulling it all the way to the goal. Retrying is enabled by applying the learned registration to both camera views (here we only show the front view).}\relax }}{14}{figure.caption.23}}
\newlabel{fig:push_grasp}{{16}{14}{\small {Retrying behavior of our method combining prehensile and non-prehensile manipulation. In the first 4 time instants shown the robot pushes the object. It then loses the object, and decides to grasp it pulling it all the way to the goal. Retrying is enabled by applying the learned registration to both camera views (here we only show the front view).}\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Left: Task setup with green dot marking the obstacle. Right, first row: the predicted frames generated by SNA. Second row: the probability distribution of the designated pixel on the \textit  {moving} object (brown stuffed animal). Note that part of the distribution shifts down and left, which is the indicated goal. Third row: the probability distribution of the designated pixel on the obstacle-object (blue power drill). Although the distribution increases in entropy during the occlusion (in the middle), it then recovers and remains on its original position. \relax }}{14}{figure.caption.24}}
\newlabel{fig:goingaroundocclusion}{{17}{14}{Left: Task setup with green dot marking the obstacle. Right, first row: the predicted frames generated by SNA. Second row: the probability distribution of the designated pixel on the \textit {moving} object (brown stuffed animal). Note that part of the distribution shifts down and left, which is the indicated goal. Third row: the probability distribution of the designated pixel on the obstacle-object (blue power drill). Although the distribution increases in entropy during the occlusion (in the middle), it then recovers and remains on its original position. \relax }{figure.caption.24}{}}
