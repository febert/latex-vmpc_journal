\section{Related Work}\label{sec:rel_work}

\subsection{Large-Scale, Self-Supervised Robotic Learning}

In a number of recent works large-scale robotic data collection (often using multiple robots) has been the key factor enabling the automatic acquisition of generalizable features and skills. Several prior works have focused on autonomous data collection for individual skills, such as grasping~\cite{lerrel,google_handeye} or obstacle avoidance~\cite{greg_kahn_uncertainty,crashing}. In contrast to these methods, our approach learns predictive models that can be used to perform a variety of manipulations, and does not require a success measure or reward function during data collection. The prior method \cite{pulkit} sought to learn an inverse from raw sensory data without any supervision.
While these methods demonstrated effective generalization to new objects, they were limited in the complexity of tasks and time-scale at which these tasks could be performed. The method proposed by~\cite{pulkit} was able to plan single pokes, and then greedily execute multiple pokes in sequence. We observe a substantial improvement in the length and complexity of manipulations that can be performed with our models.

\subsection{Sensory Prediction Models}

Action-conditioned video prediction has been explored in the context of synthetic video game images~\cite{atarioh,recurrentsimulators} and robotic manipulation~\cite{bootsetal,finn_nips,video_pixel_networks}, and video prediction without actions has been studied for unstructured videos~\cite{beyond_mse,convlstm,vondrick} and driving~\cite{prednet,dynamic_filter_networks}. 

Several works have sought to use more complex distributions for future images, for example by using autoregressive models~\cite{video_pixel_networks,scott_reed}. While this often produces sharp predictions, the resulting models are extremely demanding computationally, and have not been applied to real-world robotic control. In this work, we extend video prediction methods that are based on predicting a transformation from the previous image~\cite{finn_nips,dynamic_filter_networks}. Prior work has also sought to predict motion directly in 3D, using 3D point clouds obtained from a depth camera~\cite{se3}, requiring point-to-point correspondences over time, which makes it hard to apply to previously unseen objects. Our predictive model is effective for a wide range of real-world object manipulations and does not require 3D depth sensing or point-to-point correspondences between frames.
Prior work has also proposed to plan through learned models via differentiation, though not with visual inputs~\cite{deep_mpc}. We instead use a stochastic, sampling-based planning method~\cite{cem-rk-13,foresight}, which we extend to handle a mixture of continuous and discrete actions.