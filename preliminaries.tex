\section{Visual Model Predictive Control}\label{sec:prelim}
\label{sec:vmpc}
In this section we define the visual model-predictive control problem formulation.  We assume that the user defines a goal in terms of pixel motion by clicking on the object in the image and a corresponding target location, or by providing one or several demonstrations. Designated pixel positions and demonstrations can also be combined. Based on either of these task definitions we define per-time step cost functions $c_t$ which are computed based on the results of the \emph{action-conditioned} video-prediction model. To find an action sequence $a_{t_0:T}$ for which $c = \sum^{T}_{t=t_0}{c_t}$ over the time steps is minimal we use sampling based planning: A large number of candidate action sequences is sampled and the model's predictions are evaluated using $c$. The first action of the sequence which achieved lowest cost is applied to the robot.

To render the planning process more efficient we use the cross-entropy method (CEM), a gradient-free optimization procedure that consists of iteratively resampling action sequences and refitting Gaussian distributions to the actions with the best predicted cost. Further details can be found in section \ref{sec:optimizer}.

To improve robustness against imperfect models, the actions are iteratively replanned at each real-world time step $\tau \in \{0,...,\tau_{max}\}$ following the framework of model-predictive control (MPC): at each real-world step\footnote{With real-world step we mean timestep of the real-world as opposed to predicted timesteps.} $\tau$, the model is used to plan $T$ steps into the future, and the first action of the plan is executed.




