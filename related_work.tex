\section{Related Work}\label{sec:rel_work}

\textbf{Learning without access to rewards.}
Reinforcement learning typically assumes access to an external reward signal, effectively \emph{reinforcing} good behavior \cite{lillicrap2015continuous, sutton1998reinforcement}. Our approach learns a \emph{goal-agnostic} model without any external rewards, and then uses this model to plan to achieve user-specified goals at test-time.
A wide range of tasks can be solved by defining appropriate \emph{internal} cost functions. These internal costs are computed based on rollouts of the sensory prediction model, as detailed in section \ref{sec:cost}. In fact there has been several recent works on RL formulations that do not require external rewards\cite{chentanez2005intrinsically, pathak2017curiosity}, or reduce the necessity for them \cite{andrychowicz2017hindsight}.
%the above paragraph should have some citations -- what's the point of having a related work paragraph that doesn't actually summarize any related work?
%Thus our method is reminiscent of model-predictive control (MPC), but instead of an analytical, hand-designed model, we employ a deep sensory prediction model.

%%SL.10.15: We need a paragraph about model-based RL somewhere here. Maybe something like this:
\noindent \textbf{Model-based reinforcement learning.} Learning a model to predict the future, and then using this model to act, falls under the general umbrella of model-based reinforcement learning. Model-based algorithms are generally known to be more efficient than model-free methods~\cite{chua2018deep, deisenroth2013survey}, and have been used with both low-dimensional~\cite{deisenroth2011pilco} and high-dimensional~\cite{nagabandi2017neural} model classes. However, model-based RL methods that directly operate on raw image frames have not been studied as extensively. Several algorithms have been proposed for simple, synthetic images~\cite{watter2015embed} and video game environments~\cite{alexey, ha2018world, atarioh}, but have not been evaluated on generalization or in the real world, and recent work has also studied model-based RL for individual robotic skills~\cite{zhang2018solar}. In contrast to these works, we place special emphasis on \emph{generalization}, studying how predictive models can enable a real robot to manipulate previously unseen objects.
%%CF.11.04: Inverse models should go in the model-based RL section, I think.
%%SL.11.11: done
Several prior works have also sought to learn inverse models that map from pairs of observations to actions, which can then be used greedily to carry out short-horizon tasks~\cite{agrawal2016learning,nair2017combining, ropes}. However, such methods do not directly construct longer-term plans, relying instead on greedy execution. In contrast, our method learns a forward model, which can be used to plan out a sequence of actions to achieve a user-specified goal.

%Prior work has also used \emph{inverse models}, which learn to predict actions that lead from one st%ate to another state specified by sensory inputs, to learn skills such as object pushing~\cite{agrawal2016learning} and rope tying~\cite{nair2017combining, ropes}.
%Pinto et al. \cite{pinto2016curious} perform self-supervised learning in a multi-task setting. However, such methods struggle when applied to more extended or continuous tasks, where their inability to support forward planning and tendency to pick up on extraneous distractors, e.g., predicting the action just from observing the arm, present severe challenges. Therefore most of these approaches need to remove the arm from the scene after each action -- negatively impacting execution speed.
%%SL.10.15: is there any evidence (at least in sim?) we can show to back up this last assertion?
%Thanard's model:
%Self-supervised learning has also been used in the context of learning a latent-space describing the environment states, in which planning can be performed \cite{kurutach2018learning}.
%%CF.11.04: don't forget to address the above.
%%SL.11.11: cut the above, moved to sensory prediction (it's not a robot learning paper, because there is no robot learning)

\noindent \textbf{Self-supervised robotic learning.}
A number of recent works have studied self-supervised robotic learning, where large-scale unattended data collection is used to learn individual skills such as grasping~\cite{mottaghi2016happens, lerrel,google_handeye, calandra2017feeling,pinto2016curious} or obstacle avoidance~\cite{greg_kahn_uncertainty,crashing}. %%CF.11.04: Add Vladlen's "learning to act by predicting the future" paper to the above sentence, which also predicts future "events".
%%SL: moved to previous section
%In \cite{alexey} a goal-conditioned and action-conditional predictive model is learned-self self-supervised, applied to the context of video-games.
In contrast to these methods, our approach learns predictive models that can be used to perform a variety of manipulation skills, and does not require a success measure, event indicator, or reward function during data collection.

\noindent \textbf{Sensory prediction models.}
We propose to leverage sensory prediction models, such as video-prediction models, to enable large-scale self-supervised learning of robotic skills. Prior work on video prediction has studied predicting synthetic video game images~\cite{atarioh,recurrentsimulators} and real-world images~\cite{bootsetal,finn_nips,video_pixel_networks}, using both direct autoregressive frame prediction~\cite{beyond_mse,finn_nips,video_pixel_networks} and latent variable models~\cite{zhang2018solar,kurutach2018learning}. Video prediction without actions has been studied for unstructured videos~\cite{beyond_mse,convlstm,vondrick} and driving~\cite{prednet,dynamic_filter_networks}, as well as more structured 3D point cloud data~\cite{se3}. Several works have sought to use more complex distributions for future images, for example by using autoregressive models~\cite{video_pixel_networks,scott_reed}. While this often produces sharp predictions, the resulting models are extremely demanding computationally. In this work, we extend video prediction methods that are based on predicting a transformation from the previous image~\cite{finn_nips,dynamic_filter_networks}. 
%rior work has also sought to predict motion directly in 3D, using 3D point clouds obtained from a depth camera~\cite{se3}, requiring point-to-point correspondences over time, which makes it hard to apply to previously unseen objects.
Our predictive model is effective for a wide range of real-world object manipulations and operates directly on RGB images. In contrast to prior works that focus primarily on prediction accuracy, we demonstrate that our models enable real-world robotic control, making it possible to perform a variety of user-specified manipulation tasks.
%%CF.11.04: maybe also state that advances in video prediction are complementary to our approach. FE: I would rather mention this in the disucssion in the end.
