\section{Related Work}\label{sec:rel_work}

%%SL.10.15: changed subsection to \paragraph -- don't have hanging subsections without any text between \section and \subsection
\textbf{Learning from rewards.}
Reinforcement learning typically assumes access to an external reward signal, effectively \emph{reinforcing} good behavior \cite{lillicrap2015continuous, sutton1998reinforcement}. Our approach learns a \emph{goal-agnostic} model without any external rewards, and then uses this model to plan to achieve user-specified goals at test-time.
A wide range of tasks can be solved by defining appropriate \emph{internal} cost functions. These internal costs are computed based on rollouts of the sensory prediction model, as detailed in section \ref{sec:cost}.
%%SL.10.15: This is not the place to discuss MPC, because MPC does not learn from rewards. However, the above paragraph should have some citations -- what's the point of having a related work paragraph that doesn't actually summarize any related work?
%Thus our method is reminiscent of model-predictive control (MPC), but instead of an analytical, hand-designed model, we employ a deep sensory prediction model.

%%SL.10.15: We need a paragraph about model-based RL somewhere here. Maybe something like this:
\noindent \textbf{Model-based reinforcement learning.} Learning a model to predict the future, and then using this model to act, falls under the general umbrella of model-based reinforcement learning. Model-based algorithms are generally known to be more efficient than model-free methods~\cite{chua2018deep, deisenroth2013survey}, and have been used with both low-dimensional~\cite{deisenroth2011pilco} and high-dimensional~\cite{nagabandi2017neural} model classes. However, model-based RL methods that directly operate on raw image frames have not been studied as extensively. Several algorithms have been proposed for simple, synthetic images~\cite{watter2015embed} and video game environments~\cite{ha2018world}, but have not been evaluated on generalization or in the real world, and recent work has also studied model-based RL for individual robotic skills~\cite{zhang2018solar}. In contrast to these works, we place special emphasis on \emph{generalization}, studying how predictive models can enable a real robot to manipulate previously unseen objects.

\noindent \textbf{Self-supervised robotic learning.}
A number of recent works have studied self-supervised robotic learning, where large-scale unattended data collection is used to learn individual skills such as grasping~\cite{mottaghi2016happens, lerrel,google_handeye} or obstacle avoidance~\cite{greg_kahn_uncertainty,crashing}. 
 reviewers. Also cite Roberto's paper probably.
In contrast to these methods, our approach learns predictive models that can be used to perform a variety of manipulation skills, and does not require a success measure or reward function during data collection. 

Prior work has also used ``inverse models,'' which learn to predict actions that lead from one state to another state specified by sensory inputs, to learn skills such as object pushing~\cite{agrawal2016learning} and rope tying~\cite{nair2017combining}.
Pinto et al. \cite{pinto2016curious} perform self-supervised learning in a multi-task setting combing learning an inverse model for pushing, a grasp success metric and pose-invariant image embeddings.
%%SL.10.15: This should cite Ashvin's 2017 ICRA paper on rope manipulation.
However, such methods struggle when applied to more extended or continuous tasks, where their inability to support forward planning and tendency to pick up on extraneous distractors (e.g., predicting the action just from observing the arm) present severe challenges. We observe a substantial improvement in the length and complexity of manipulations that can be performed with our method.
%%SL.10.15: is there any evidence (at least in sim?) we can show to back up this last assertion?

%Thanard's model:
Kurutach et al. use a InfoGAN model \cite{kurutach2018learning} to learn a latent-space describing the environment states in which planning can be performed. The model can be used to obtain a sequence of waypoints in the latent space between the current state and goal-state. The method currently relies on an inverse model to reach the states proposed by the InfoGAN model. So far experiments have been limited to rope rearrangement tasks.
%%SL.10.15: I don't think that paper has any rope rearrangement, just some results showing predictions? But maybe more to the point, we should probably just have a broader discussion of video prediction models, and put this citation somewhere in the middle of that. It seems weird to single it out among the other predictive models papers. I would just suggest deleting this paragraph and working the citation somewhere into the next paragraph.

\noindent \textbf{Sensory prediction models.}
We propose to leverage sensory prediction models, such as video-prediction models, to enable large-scale self-supervised learning of robotic skills. Prior work on video prediction has studied synthetic video game images~\cite{atarioh,recurrentsimulators} and robotic manipulation~\cite{bootsetal,finn_nips,video_pixel_networks}. Video prediction without actions has been studied for unstructured videos~\cite{beyond_mse,convlstm,vondrick} and driving~\cite{prednet,dynamic_filter_networks}. Several works have sought to use more complex distributions for future images, for example by using autoregressive models~\cite{video_pixel_networks,scott_reed}. While this often produces sharp predictions, the resulting models are extremely demanding computationally which would be impractical for real-world robotic control. In this work, we extend video prediction methods that are based on predicting a transformation from the previous image~\cite{finn_nips,dynamic_filter_networks}. Prior work has also sought to predict motion directly in 3D, using 3D point clouds obtained from a depth camera~\cite{se3}, requiring point-to-point correspondences over time, which makes it hard to apply to previously unseen objects. Our predictive model is effective for a wide range of real-world object manipulations and does not require 3D depth sensing or point-to-point correspondences between frames. Crucially, we demonstrate that our models enable real-world robotic control, making it possible to perform a variety of user-specified manipulation tasks.
