\section{Trajectory Optimizer}

\begin{algorithm}[ht]
\caption{Trajecotry Optimization in Visual MPC}
\label{sarsalambdafa}
\begin{algorithmic}[1]
\State Initialize $\bm w$ arbitrarily as an array of size $\#actions \cdot \#tiles$
\For{each episode}
\State $E \leftarrow E + \bm x(S,A)$
\While{$S$ is not terminal}
\State Take action $A$, observe $R$, $S'$
\State $S \leftarrow S', A \leftarrow A'$
\EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}



\label{sec:optimizer}
The role of the optimizer is to find actions sequences $a_{1:T}$ which minimize the sum of the costs $c_{1:T}$ along the planning horizon $T$ by sampling a large number of actions sequences and ranking of each video-prediction rollout using a cost function.

To render the planning process more efficient, we use the cross-entropy method (CEM), a gradient-free optimization procedure.
CEM consists of iteratively resampling action sequences and refitting Gaussian distributions to the actions with the best predicted cost. 
We extend CEM to handle a mixture of continuous and discrete actions.

The motivation for using a gradient-free, sampling-based optimizer is that in this way we can easily ensure that actions stay within the distribution of actions the model has encountered during training. This is crucial to ensure that the model does not receive out-of-distribution inputs and makes valid predictions. 

In the appendix \ref{sec:cem_improv} we explain a few improvements we made to the CEM-optimizer.