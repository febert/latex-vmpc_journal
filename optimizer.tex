\section{Trajectory Optimizer}
\label{sec:optimizer}

%%SL.10.15: It's bad form to begin a technical section with prior work. Prior work belongs in the related work section.
Prior work has also proposed to plan through learned models via differentiation, though not with visual inputs~\cite{deep_mpc}. We instead use a stochastic, sampling-based trajectory optimization method~\cite{cem-rk-13,foresight}, which we extend to handle a mixture of continuous and discrete actions.

The role of the optimizer is to find actions sequences $a_{1:T}$ which minimize the sum of the costs $c_{1:T}$ along the planning horizon $T$. This is achieved via iterative sampling using the Cross-Entropy Method
%%SL.10.15: algorithm names are not capitalized
and ranking of each video-prediction rollout using a cost function. 

In the model-predictive control setting, the action sequences found by the optimizer can be very different between execution real-world times steps. For example at one time step the optimizer might find a pushing action leading towards the goal and in the next time step it determines a grasping action to be optimal to reach the goal. Na\"{i}ve replanning at every time step can then result in alternating between a pushing and a grasping attempt indefinitely causing the agent to get stuck and not making any progress towards to goal. 

%%SL.10.15: It's not clear whether this is done in every experiment or just some experiments.
We can resolve this problem by modifying the sampling distribution of the first iteration of CEM so that the optimizer commits to the plan found in the previous time step. In prior work \cite{sna}
%%SL.10.15: that's not really prior work for this paper...
the sampling distribution at first iteration of CEM is chosen to be a Gaussian with diagonal covariance matrix and zero mean. We instead use the best action sequence found in the optimization of the \emph{previous} real-world time step as the mean for sampling new actions in the \emph{current} real-world time-step. Since this action sequence is optimized for the previous time step we only use the values $a_{2:T}$ and omit the first action. To sample actions close to the action sequence from the previous time step we reduce the entries of the diagonal covariance matrix for the first $T-1$ time steps. It is crucial that the last entry of the covariance matrix at the end of the horizon is not reduced otherwise no exploration could happen for the last time step causing poor performance at later time steps.