\section{Overview}\label{sec:prelim}
\label{sec:vmpc}
%%SL.10.15: change title to Overview?

%%SL.10.15: This is the wrong level of abstraction for this section. This section should provide a high-level overview of the structure of the method: data collection (how is that done?), model training, some considerations about the model (what is input and what is output?) and a brief summary of the test-time control method, not at the level of designated pixels yet, but maybe with a short summary of different ways to specify costs and some details about planning. I recommend starting with a high level motivation sentence or two, and then having \textbf{} paragraph headings for "Training data collection" "Model training" and "Test-time control", then briefly summarize what the following sections will be about, something like "In the next section, we will discuss the architectures and training procedures for our predictive models, followed by a discussion of planning objectives in Section~\ref{sec:something}, planning in Section~\ref{sec:something}, and system design considerations in Section~\ref{sec:something}." Here is an example for how you can open with the high level motivation: 
In this section, we summarize our visual model-predictive control (MPC) method, which is a model-based reinforcement learning approach to end-to-end learning of robotic manipulation skills. Our method, outlined in Figure \ref{fig:overview}, consists of three phases: unsupervised data collection, predictive model training, and planning-based control via the model at test-time.

\noindent \textbf{Unsupervised data collection}: At \emph{training time}, data is collected autonomously by applying random actions sampled from a pre-specified distribution. It is important that this distribution allows the robot to visit parts of the state space that are relevant for solving the intended tasks. For some tasks, uniform random actions are sufficient, while for others, the design of the exploration strategy takes additional care, as detailed in Sections \ref{sec:system} and \ref{subsec:cloth_folding_data}.

\noindent \textbf{Model training}: We train a video prediction model offline on the collected data. The video prediction model takes as input an image of the current timestep and a sequence of actions, and generates the corresponding sequence of future frames. This model is described in Section \ref{sec:model}.

\noindent \textbf{Test time control}: At \emph{test time}, we use a sampling-based, gradient free optimization procedure, similar to a shooting method,
%%SL.11.22: add citation
to find the sequence of actions that minimizes a cost function. Further details, including the motivation for this type of optimizer, can be found in Section \ref{sec:optimizer}. 

Depending on how the goal is specified, we use one of the following three cost functions. When the goal is provided by clicking on an object and a desired goal-position, a \emph{pixel-distance cost-function}, detailed in Section \ref{subsec:pixel_dist_cost}, evaluates how far the designated pixel is from the goal pixels. We can specify the goal more precisely by providing a goal image in addition to the pixel positions and make use of \emph{image-to-image registration} to compute a cost function, as discussed in Section \ref{subsec:reg_cost}. Finally, we show that more abstract
%%SL.11.22: the use of the word "abstract" here doesn't really convey what this means. Maybe that's the best we can do, but can you think of a better way we could express this?
tasks can be solved by providing one or several demonstrations and employing a \emph{classifier-based} cost function as detailed in Section \ref{subsec:class_cost}. The strengths and weaknesses of different costs functions and trade-offs between them are discussed in Section \ref{subsec:cost_discuission}. 

The model is used to plan $T$ steps into the future, and the first action of the action sequence that attained lowest cost, is executed. In order to correct for mistakes made by the model, the actions are iteratively replanned at each real-world time step\footnote{With real-world step we mean timestep of the real-world as opposed to predicted timesteps.} $\tau \in \{0,...,\tau_{max}\}$ following the framework of model-predictive control (MPC). 
In the following sections, we explain the video-prediction model, the planning cost function, and the trajectory optimizer.


% We assume that the user defines a goal in terms of pixel motion by clicking on the object in the image and a corresponding target location, or by providing one or several demonstrations. Designated pixel positions and demonstrations can also be combined. Based on either of these task definitions we define per-time step cost functions $c_t$ which are computed based on the results of the \emph{action-conditioned} video-prediction model. To find an action sequence $a_{t_0:T}$ for which $c = \sum^{T}_{t=t_0}{c_t}$ over the time steps is minimal we use sampling based planning: A large number of candidate action sequences is sampled and the model's predictions are evaluated using $c$. The first action of the sequence which achieved lowest cost is applied to the robot.






